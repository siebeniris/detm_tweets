{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef3da137-8520-47ee-9fe6-662d712eb8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '/home/yiyi/nlp_tm/models/DETM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40629c03-0119-43e1-9d34-cfe714699730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320d1bd6-88a6-42ac-bd86-ce00df23befb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import pickle \n",
    "import numpy as np \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import sys\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import scipy.io\n",
    "\n",
    "import data \n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from detm import DETM\n",
    "from utils import nearest_neighbors, get_topic_coherence\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6683b1-38af-4ddb-b3a6-40eb781dd75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5723d959-d8d9-4b23-a160-a17b82d77a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir ='/home/yiyi/nlp_tm/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba0447-4351-452b-8d28-6d95cd235d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 10\n",
    "filepath = os.path.join(root_dir, f'results/detm_twitter_K_10_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_300_L_3_minDF_10_trainEmbeddings_1_val_ppl_2757.5_epoch_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac808b9a-8358-4c71-8c00-1f4fd59e7b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join('/home/yiyi/nlp_tm/', filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e9bb32-cdf4-4fda-8f48-9ab0fa020879",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_path, 'rb') as f:\n",
    "    model = torch.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eebeae-01a6-496a-ba05-13772a5eab78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d2a831-e855-42dc-8c68-1b1657a0b12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import nearest_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837d3286-8df8-4f82-9ce6-cbb44da08a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('/home/yiyi/nlp_tm/preprocessed_data', 'vocab.pkl'), 'rb') as f:\n",
    "        vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b0158b-d9c6-4f47-85df-208cd70b27a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754d5ee4-676d-429e-b0c5-7e97d144406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(input_dir= os.path.join(root_dir, 'preprocessed_data')):\n",
    "    token_file = os.path.join(input_dir, 'bow_tokens')\n",
    "    count_file = os.path.join(input_dir, 'bow_counts')\n",
    "    time_file = os.path.join(input_dir, 'bow_timestamps')\n",
    "    tokens = scipy.io.loadmat(token_file)['tokens'].squeeze()\n",
    "    counts = scipy.io.loadmat(count_file)['counts'].squeeze()\n",
    "    times = scipy.io.loadmat(time_file)['timestamps'].squeeze()\n",
    "    \n",
    "    \n",
    "    return tokens, counts, times\n",
    "\n",
    "def _eta_helper(rnn_inp):\n",
    "    inp = model.q_eta_map(rnn_inp).unsqueeze(1)\n",
    "    hidden = model.init_hidden()\n",
    "    output, _ = model.q_eta(inp, hidden)\n",
    "    output = output.squeeze()\n",
    "    etas = torch.zeros(model.num_times, model.num_topics).to(device)\n",
    "    inp_0 = torch.cat([output[0], torch.zeros(model.num_topics,).to(device)], dim=0)\n",
    "    etas[0] = model.mu_q_eta(inp_0)\n",
    "    for t in range(1, model.num_times):\n",
    "        inp_t = torch.cat([output[t], etas[t-1]], dim=0)\n",
    "        etas[t] = model.mu_q_eta(inp_t)\n",
    "    return etas\n",
    "\n",
    "def get_theta(eta, bows):\n",
    "    \"\"\"\n",
    "    Document proportions....\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inp = torch.cat([bows, eta], dim=1)\n",
    "        q_theta = model.q_theta(inp)\n",
    "        mu_theta = model.mu_q_theta(q_theta)\n",
    "        theta = F.softmax(mu_theta, dim=-1)\n",
    "        return theta \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac172bda-bf2b-4bb8-aa14-38a0c60a212b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DETM(\n",
       "  (t_drop): Dropout(p=0.0, inplace=False)\n",
       "  (theta_act): ReLU()\n",
       "  (rho): Linear(in_features=300, out_features=20865, bias=False)\n",
       "  (q_theta): Sequential(\n",
       "    (0): Linear(in_features=20875, out_features=800, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=800, out_features=800, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (mu_q_theta): Linear(in_features=800, out_features=10, bias=True)\n",
       "  (logsigma_q_theta): Linear(in_features=800, out_features=10, bias=True)\n",
       "  (q_eta_map): Linear(in_features=20865, out_features=200, bias=True)\n",
       "  (q_eta): LSTM(200, 200, num_layers=3)\n",
       "  (mu_q_eta): Linear(in_features=210, out_features=10, bias=True)\n",
       "  (logsigma_q_eta): Linear(in_features=210, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9e742b-7a76-45a0-a8a7-9eec6bf58dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, counts, times = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b815653-df45-4c42-b90e-3b11f6310697",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360097 360097 360097\n",
      "documents nr:  360097\n",
      "idx: 0/361\n",
      "idx: 20/361\n",
      "idx: 40/361\n",
      "idx: 60/361\n",
      "idx: 80/361\n",
      "idx: 100/361\n",
      "idx: 120/361\n",
      "idx: 140/361\n",
      "idx: 160/361\n",
      "idx: 180/361\n",
      "idx: 200/361\n",
      "idx: 220/361\n",
      "idx: 240/361\n",
      "idx: 260/361\n",
      "idx: 280/361\n",
      "idx: 300/361\n",
      "idx: 320/361\n",
      "idx: 340/361\n",
      "idx: 360/361\n",
      "0 1000\n",
      "tensor([[-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        ...,\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392]],\n",
      "       device='cuda:3')\n",
      "torch.Size([1000, 10, 20865])\n",
      "1 1000\n",
      "tensor([[-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        ...,\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392]],\n",
      "       device='cuda:3')\n",
      "torch.Size([1000, 10, 20865])\n",
      "2 1000\n",
      "tensor([[-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        ...,\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392]],\n",
      "       device='cuda:3')\n",
      "torch.Size([1000, 10, 20865])\n",
      "3 1000\n",
      "tensor([[-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        ...,\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392]],\n",
      "       device='cuda:3')\n",
      "torch.Size([1000, 10, 20865])\n",
      "4 1000\n",
      "tensor([[-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        ...,\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392]],\n",
      "       device='cuda:3')\n",
      "torch.Size([1000, 10, 20865])\n",
      "5 1000\n",
      "tensor([[-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        ...,\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392]],\n",
      "       device='cuda:3')\n",
      "torch.Size([1000, 10, 20865])\n",
      "6 1000\n",
      "tensor([[-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        ...,\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392]],\n",
      "       device='cuda:3')\n",
      "torch.Size([1000, 10, 20865])\n",
      "7 1000\n",
      "tensor([[-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        ...,\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392]],\n",
      "       device='cuda:3')\n",
      "torch.Size([1000, 10, 20865])\n",
      "8 1000\n",
      "tensor([[-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        ...,\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392]],\n",
      "       device='cuda:3')\n",
      "torch.Size([1000, 10, 20865])\n",
      "9 1000\n",
      "tensor([[-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        ...,\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392]],\n",
      "       device='cuda:3')\n",
      "torch.Size([1000, 10, 20865])\n",
      "10 1000\n",
      "tensor([[-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        ...,\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392]],\n",
      "       device='cuda:3')\n",
      "torch.Size([1000, 10, 20865])\n",
      "11 1000\n",
      "tensor([[-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        ...,\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392]],\n",
      "       device='cuda:3')\n",
      "torch.Size([1000, 10, 20865])\n",
      "12 1000\n",
      "tensor([[-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        ...,\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392],\n",
      "        [-0.7551,  0.0196, -0.0380,  ...,  1.4528, -0.8639,  0.3392]],\n",
      "       device='cuda:3')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 796.00 MiB (GPU 3; 11.91 GiB total capacity; 10.38 GiB already allocated; 207.06 MiB free; 11.09 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-db05a85a3372>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m############ get topic words#####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0malpha_td\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimes_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'torch.LongTensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_beta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha_td\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nlp_tm/models/DETM/detm.py\u001b[0m in \u001b[0;36mget_beta\u001b[0;34m(self, alpha)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hatex/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1512\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1513\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 796.00 MiB (GPU 3; 11.91 GiB total capacity; 10.38 GiB already allocated; 207.06 MiB free; 11.09 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "tokens, counts, times = load_data()\n",
    "print(len(tokens), len(counts), len(times))\n",
    "print('documents nr: ', len(tokens))\n",
    "indices = torch.split(torch.tensor(range(len(tokens))), 1000)\n",
    "\n",
    "#     csvfile = open(f'topic_visualization_topnum{num_topics}.csv', 'w', newline='')\n",
    "#     csvwriter = csv.writer(csvfile, delimiter= ',', quoting=csv.QUOTE_MINIMAL)\n",
    "#     csvwriter.writerow(['Topic', 'Time', 'Topic Words'])\n",
    "\n",
    "# theta_weights=[]\n",
    "\n",
    "\n",
    "timeslist = [x for x in range(8)]\n",
    "num_topics =10\n",
    "num_words =10\n",
    "# beta_arr = np.empty((len(tokens), num_topics, len(vocab)))\n",
    "# gamma_dict = defaultdict(dict)\n",
    "# for topic in range(0, 10):\n",
    "#     gamma_dict[topic]={year:{} for year in range(0,8)}\n",
    "#     for year in range(0,8):\n",
    "#         gamma_dict[topic][year]= {x:0 for x in range(0, len(vocab))}\n",
    "beta_rows = []\n",
    "        \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    alpha = model.mu_q_alpha\n",
    "    rnn_inp = data.get_rnn_input(tokens, counts, times, 8, len(vocab), len(tokens))\n",
    "    rnn_inp = rnn_inp.to(device)\n",
    "    etas = _eta_helper(rnn_inp)\n",
    "\n",
    "\n",
    "    for idx, ind in enumerate(indices):\n",
    "        print(idx, len(ind))\n",
    "        \n",
    "        data_batch, times_batch = data.get_batch(\n",
    "                        tokens, counts, ind, len(vocab),300, temporal=True, times=times)\n",
    "        sums = data_batch.sum(1).unsqueeze(1)\n",
    "\n",
    "        normalized_data_batch = data_batch / sums\n",
    "\n",
    "        eta_td = etas[times_batch.type('torch.LongTensor')]\n",
    "        print(eta_td)\n",
    "        ############ get topic words#####\n",
    "        alpha_td = alpha[:, times_batch.type('torch.LongTensor'), :].to(device)\n",
    "        beta = model.get_beta(alpha_td).permute(1, 0, 2)\n",
    "        print(beta.shape)\n",
    "        for i in beta:\n",
    "            beta_rows.append(i)\n",
    "#         for k in range(num_topics):\n",
    "#             for t in timeslist:\n",
    "#                 gamma =beta[k,t,:]\n",
    "#                 sorted_ids = g.argsort()[-num_words:][::-1]\n",
    "#                 sorted_scores = np.sort(g)[-num_words:][::-1]\n",
    "#                 d= zip(sorted_ids, sorted_scores)\n",
    "#                 gamma_dict[k][t].update(d)\n",
    "\n",
    "#                 del(gamma)\n",
    "    \n",
    "                \n",
    "#         LEN = beta.shape[0]\n",
    "#         print(idx*LEN, ':', (idx+1)*LEN)\n",
    "#         beta_arr[idx*LEN:(idx+1)*LEN]= beta\n",
    "                    \n",
    "#         theta = get_theta(eta_td, normalized_data_batch).to(device)\n",
    "#         theta = theta.cpu().numpy()\n",
    "#         theta_weights.append(theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67150e24-ed80-40a3-bf91-4389a993f4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_arr = np.array(beta_rows, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1a02dd9-2457-420c-9057-024999caf8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360097, 10, 20865)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f15afadf-86cd-4b35-b814-7fb081c12c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_arr[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7101d7b-4dea-47b5-b326-15829da628b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 .. Time: 0 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 0 .. Time: 1 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 0 .. Time: 2 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 0 .. Time: 3 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 0 .. Time: 4 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 0 .. Time: 5 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 0 .. Time: 6 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 0 .. Time: 7 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 1 .. Time: 0 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 1 .. Time: 1 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 1 .. Time: 2 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 1 .. Time: 3 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 1 .. Time: 4 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 1 .. Time: 5 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 1 .. Time: 6 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 1 .. Time: 7 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 2 .. Time: 0 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 2 .. Time: 1 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 2 .. Time: 2 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 2 .. Time: 3 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 2 .. Time: 4 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 2 .. Time: 5 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 2 .. Time: 6 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 2 .. Time: 7 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 3 .. Time: 0 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 3 .. Time: 1 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 3 .. Time: 2 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 3 .. Time: 3 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 3 .. Time: 4 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 3 .. Time: 5 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 3 .. Time: 6 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 3 .. Time: 7 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 4 .. Time: 0 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 4 .. Time: 1 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 4 .. Time: 2 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 4 .. Time: 3 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 4 .. Time: 4 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 4 .. Time: 5 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 4 .. Time: 6 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 4 .. Time: 7 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 5 .. Time: 0 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 5 .. Time: 1 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 5 .. Time: 2 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 5 .. Time: 3 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 5 .. Time: 4 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 5 .. Time: 5 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 5 .. Time: 6 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 5 .. Time: 7 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 6 .. Time: 0 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 6 .. Time: 1 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 6 .. Time: 2 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 6 .. Time: 3 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 6 .. Time: 4 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 6 .. Time: 5 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 6 .. Time: 6 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 6 .. Time: 7 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 7 .. Time: 0 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 7 .. Time: 1 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 7 .. Time: 2 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 7 .. Time: 3 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 7 .. Time: 4 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 7 .. Time: 5 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 7 .. Time: 6 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 7 .. Time: 7 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 8 .. Time: 0 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 8 .. Time: 1 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 8 .. Time: 2 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 8 .. Time: 3 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 8 .. Time: 4 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 8 .. Time: 5 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 8 .. Time: 6 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 8 .. Time: 7 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 9 .. Time: 0 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 9 .. Time: 1 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 9 .. Time: 2 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 9 .. Time: 3 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 9 .. Time: 4 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 9 .. Time: 5 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 9 .. Time: 6 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n",
      "Topic 9 .. Time: 7 ===> ['indonesia', 'boozer', 'migraine', 'baltics', 'role', 'ammunition', 'europemustact', 'prater', 'taint']\n"
     ]
    }
   ],
   "source": [
    "for k in range(num_topics):\n",
    "    for t in timeslist:\n",
    "        gamma = beta_arr[k, t, :]\n",
    "        top_words = list(gamma.argsort()[-num_words+1:][::-1])\n",
    "        topic_words = [vocab[a] for a in top_words]\n",
    "#         topics_words.append(' '.join(topic_words))\n",
    "\n",
    "#         csvwriter.writerow([k, t+2013, topic_words])\n",
    "\n",
    "        print('Topic {} .. Time: {} ===> {}'.format(k, t, topic_words)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db96ecd-7da7-485e-8588-41233784d7fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bbd6ed-74c0-414e-9ed4-0c1d4e7b9660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def visualize(num_topics, vocab):\n",
    "    \"\"\"Visualizes topics and embeddings and word usage evolution.\n",
    "    \"\"\"\n",
    "    tokens, counts, times = load_data()\n",
    "    print(len(tokens), len(counts), len(times))\n",
    "    print('documents nr: ', len(tokens))\n",
    "    indices = torch.split(torch.tensor(range(len(tokens))), 1000)\n",
    "    \n",
    "#     csvfile = open(f'topic_visualization_topnum{num_topics}.csv', 'w', newline='')\n",
    "#     csvwriter = csv.writer(csvfile, delimiter= ',', quoting=csv.QUOTE_MINIMAL)\n",
    "#     csvwriter.writerow(['Topic', 'Time', 'Topic Words'])\n",
    "    \n",
    "    theta_weights=[]\n",
    "    beta_weights=[]\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        alpha = model.mu_q_alpha\n",
    "        rnn_inp = data.get_rnn_input(tokens, counts, times, 8, len(vocab), len(tokens))\n",
    "        etas = _eta_helper(rnn_inp)\n",
    "        \n",
    "                \n",
    "        for idx, ind in enumerate(indices):\n",
    "            data_batch, times_batch = data.get_batch(\n",
    "                            tokens, counts, ind, len(vocab),300, temporal=True, times=times)\n",
    "            sums = data_batch.sum(1).unsqueeze(1)\n",
    "\n",
    "            normalized_data_batch = data_batch / sums\n",
    "\n",
    "            eta_td = etas[times_batch.type('torch.LongTensor')]\n",
    "            ############ get topic words#####\n",
    "            alpha_td = alpha[:, times_batch.type('torch.LongTensor'), :].to(device)\n",
    "            beta = model.get_beta(alpha_td).permute(1, 0, 2)\n",
    "            beta_weights.append(beta.cpu().numpy())\n",
    "            \n",
    "            theta = get_theta(eta_td, normalized_data_batch).to(device)\n",
    "            theta = theta.cpu().numpy()\n",
    "            theta_weights.append(theta)\n",
    "    \n",
    "    print('collected all beta weights...')\n",
    "    beta_weights_list = []\n",
    "    for w in beta_weights:\n",
    "        for i in w:\n",
    "            beta_weights_list.append(i)\n",
    "    beta_weights_arr = np.stack(beta_weights_list, axis=0)\n",
    "    print(\"beta shape: \", beta_weights_arr.shape)\n",
    "    \n",
    "    print('collected all theta weights...')\n",
    "    theta_weights_list = []\n",
    "    for w in theta_weights:\n",
    "        for i in w:\n",
    "            theta_weights_list.append(i)\n",
    "    theta_weights_arr = np.stack(theta_weights_list, axis=0)\n",
    "    print(\"theta shape: \", theta_weights_arr.shape)\n",
    "    \n",
    "    \n",
    "    return beta_weights_arr, theta_weights_arr\n",
    "#         topics_words = []\n",
    "        \n",
    "        \n",
    "#         for k in range(num_topics):\n",
    "#             for t in times:\n",
    "#                 gamma = beta[k, t, :]\n",
    "#                 top_words = list(gamma.cpu().numpy().argsort()[-num_words+1:][::-1])\n",
    "#                 topic_words = [vocab[a] for a in top_words]\n",
    "#                 topics_words.append(' '.join(topic_words))\n",
    "                \n",
    "#                 csvwriter.writerow([k, t+2013, topic_words])\n",
    "                \n",
    "#                 print('Topic {} .. Time: {} ===> {}'.format(k, t, topic_words)) \n",
    "\n",
    "#         print('\\n')\n",
    "#         print('Visualize word embeddings ...')\n",
    "#         queries = ['immigrant', 'refugee']\n",
    "#         try:\n",
    "#             embeddings = model.rho.weight  # Vocab_size x E\n",
    "#         except:\n",
    "#             embeddings = model.rho         # Vocab_size x E\n",
    "#         neighbors = []\n",
    "#         for word in queries:\n",
    "#             print('word: {} .. neighbors: {}'.format(\n",
    "#                 word, nearest_neighbors(word, embeddings, vocab, num_words)))\n",
    "#         print('#'*100)\n",
    "#         return embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2c761e-c955-48d0-8b75-e7afa4348d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6a7e9b25-6157-49ff-b588-a44cf04a6464",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 .. Time: 2013 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 0 .. Time: 2014 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 0 .. Time: 2015 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 0 .. Time: 2016 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 0 .. Time: 2017 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 0 .. Time: 2018 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 0 .. Time: 2019 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 0 .. Time: 2020 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 1 .. Time: 2013 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 1 .. Time: 2014 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 1 .. Time: 2015 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 1 .. Time: 2016 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 1 .. Time: 2017 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 1 .. Time: 2018 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 1 .. Time: 2019 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 1 .. Time: 2020 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 2 .. Time: 2013 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 2 .. Time: 2014 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 2 .. Time: 2015 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 2 .. Time: 2016 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 2 .. Time: 2017 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 2 .. Time: 2018 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 2 .. Time: 2019 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 2 .. Time: 2020 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 3 .. Time: 2013 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 3 .. Time: 2014 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 3 .. Time: 2015 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 3 .. Time: 2016 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 3 .. Time: 2017 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 3 .. Time: 2018 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 3 .. Time: 2019 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 3 .. Time: 2020 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 4 .. Time: 2013 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 4 .. Time: 2014 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 4 .. Time: 2015 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 4 .. Time: 2016 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 4 .. Time: 2017 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 4 .. Time: 2018 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 4 .. Time: 2019 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 4 .. Time: 2020 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 5 .. Time: 2013 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 5 .. Time: 2014 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 5 .. Time: 2015 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 5 .. Time: 2016 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 5 .. Time: 2017 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 5 .. Time: 2018 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 5 .. Time: 2019 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 5 .. Time: 2020 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 6 .. Time: 2013 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 6 .. Time: 2014 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 6 .. Time: 2015 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 6 .. Time: 2016 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 6 .. Time: 2017 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 6 .. Time: 2018 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 6 .. Time: 2019 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 6 .. Time: 2020 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 7 .. Time: 2013 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 7 .. Time: 2014 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 7 .. Time: 2015 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 7 .. Time: 2016 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 7 .. Time: 2017 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 7 .. Time: 2018 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 7 .. Time: 2019 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 7 .. Time: 2020 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 8 .. Time: 2013 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 8 .. Time: 2014 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 8 .. Time: 2015 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 8 .. Time: 2016 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 8 .. Time: 2017 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 8 .. Time: 2018 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 8 .. Time: 2019 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 8 .. Time: 2020 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 9 .. Time: 2013 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 9 .. Time: 2014 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 9 .. Time: 2015 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 9 .. Time: 2016 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 9 .. Time: 2017 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 9 .. Time: 2018 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 9 .. Time: 2019 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n",
      "Topic 9 .. Time: 2020 ===> ['unexpectedly', 'declaration', 'opec', 'eurovisiondebate', 'back', 'trumpmeltdown', 'shitshow', 'ungps', 'lobby', 'spanner']\n"
     ]
    }
   ],
   "source": [
    "for k in range(num_topics):\n",
    "    for t in timeslist:\n",
    "        ga = gamma_dict[k][t]\n",
    "        ga_sorted = {k:v for k,v in sorted(ga.items(), key=lambda item:item[1]) if v>0}\n",
    "        topic_words = [vocab[a] for a in ga_sorted.keys()]\n",
    "        \n",
    "        print('Topic {} .. Time: {} ===> {}'.format(k, t+2013, topic_words)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a690fd3a-c82f-4f5e-822e-e99fd04e73ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5f9ca2-910b-452c-9f8b-bd2fbde8341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(gamma[0])[::-1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f1237b-4ec8-46ae-9c67-60de493e2e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4ee662-b15e-49f2-8573-0f7c3cb7e52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('collected all beta weights...')\n",
    "beta_weights_list = []\n",
    "for w in beta_weights:\n",
    "    for i in w:\n",
    "        beta_weights_list.append(i)\n",
    "with open('k_10/beta_weights_list.pkl', 'wb') as f:\n",
    "    pickle.dump(beta_weights_list, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468deaaa-3d01-49f7-87fc-1037ad5bb1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_weights_arr = np.stack(beta_weights_list, axis=0)\n",
    "print(\"beta shape: \", beta_weights_arr.shape)\n",
    "\n",
    "print('collected all theta weights...')\n",
    "theta_weights_list = []\n",
    "for w in theta_weights:\n",
    "    for i in w:\n",
    "        theta_weights_list.append(i)\n",
    "theta_weights_arr = np.stack(theta_weights_list, axis=0)\n",
    "print(\"theta shape: \", theta_weights_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbca66b0-e068-4684-abc5-883a939661e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(beta_weights), beta_weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ac410e-02fa-4ba7-9288-d3b7b85f73b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(theta_weights), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f5719a-f6bf-4557-b1dd-d2b1671d15fe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "beta, theta = visualize(10, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afd8186-646f-4b85-859e-d4f0471353af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447eedfa-4c9f-4f7b-9f17-473af4d031d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba25bfe-6575-4874-b888-20a24b0f9a21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29ad2d6-0a9b-49d5-ab57-1a038030c133",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6e4656-9429-4636-a156-c51127426fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_ =['immigrant', 'refugee', 'immigration', 'asylum', 'exile', 'migrant', 'centroid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30b43b9-7443-40b9-b92a-4f216f61daa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3629b02-b23b-4b39-b5fc-150b635aae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors of whole embeddings.\n",
    "indexes_keywords = [vocab.index(word) for word in keywords_]\n",
    "queries = [embeddings[index] for index in indexes_keywords] # vectors of the keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5baa4f-d741-4315-aee3-23830ac474d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = cluster.KMeans(n_clusters=1, random_state=0).fit(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797b49de-bada-4a26-8261-0e2b92a5b5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid = kmeans.cluster_centers_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad61a80-f79f-4738-8949-43eb188d9a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks= embeddings.dot(centroid).squeeze()\n",
    "denom = centroid.T.dot(centroid).squeeze()\n",
    "denom = denom*np.sum(embeddings*2, 1)\n",
    "ranks = ranks/denom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33713b6-43f4-4149-8b0c-63fd491f2e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "mostSimilar = []\n",
    "[mostSimilar.append(idx) for idx in ranks.argsort()[::-1]]\n",
    "nearest_neighbors = mostSimilar[:50]\n",
    "nearest_neighbors = [vocab[comp] for comp in nearest_neighbors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73370e50-4e5f-42dc-891e-4f2b2d3784d0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "nearest_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62e9111-1ae3-4127-a473-ba8313267cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d36dfa0-6d4f-4f6a-99e1-170382c0da3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries.append(centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870493da-39a5-4e98-a8a8-e09697cb7acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e04bde-5d67-4bb8-8ece-254f6783bc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_arr = np.array(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99b958d-b845-41d8-b0f0-6535d2329e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cb396c-a4c3-47ef-9a14-ef637d2cd12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(keywords_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9563e10-3c38-4991-a61b-7ea250906e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=0, n_iter=10000, perplexity=7)\n",
    "T = tsne.fit_transform(query_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaa510f-94b4-4c59-ac36-fd68345ad146",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "plt.scatter(T[:, 0], T[:, 1])\n",
    "for label, x, y in zip(keywords_, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+2, y+2), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f74a3d-7d30-4676-82b1-6c0f935769e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a857621-13f2-407b-a7e2-f7d55da31acb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hatex",
   "language": "python",
   "name": "hatex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
